{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from stop_words import get_stop_words\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import nltk.data\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as',\n",
    "               'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \"can't\", 'cannot',\n",
    "               'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each',\n",
    "               'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\",\n",
    "               \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i',\n",
    "               \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself', \"let's\", 'me', \n",
    "               'more', 'most', \"mustn't\", 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other',\n",
    "               'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\",\n",
    "               'should', \"shouldn't\", 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves',\n",
    "               'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through',\n",
    "               'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\",\n",
    "               'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with',\n",
    "               \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves',\n",
    "               'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours',\n",
    "               'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
    "               'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\",\n",
    "               'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "               'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for',\n",
    "               'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up',\n",
    "               'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where',\n",
    "               'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only',\n",
    "               'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\",\n",
    "               'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn',\n",
    "               \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\n",
    "               \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won',\n",
    "               \"won't\", 'wouldn', \"wouldn't\",\n",
    "                'the', 'is', 'a', 'and', 'after', 'as', 'at', 'always', 'between', 'but', 'beyond', 'by', 'can',\n",
    "               'for', 'this', 'that', 'in', 'on', 'mr', 'yes', 'no', 'other', 'said', 'them', 'they', 'who', 'whom',\n",
    "               'where', 'when', 'will', 'would', 'with', 'he', 'his', 'she', 'her', 'you', 'our', 'i', 'am', 'are',\n",
    "               'about', 'been', 'able','ab','an', 'about', 'before','began','begin','not','me','some','up','too','what',\n",
    "               'better', 'did', 'do', 'feel', 'go', 'went', 'how', 'it', 'its', 'itselt', 'into', 'just', 'look', 'like',\n",
    "               'of', 'rv', 'rt', 'out', 'our', 'so', 'soon', 'there', 'these', 'those', 'r', 'to', 'want', 'was', 'we',\n",
    "               'were', 'us', 'went', 'well', 'why', 'yet','when', 'your', 'have','my','be','from','new','has','all','their',\n",
    "               'nor','or','take','took','or','if','years','more','show','here', 'th','now','one','get','facebookrt','pm','day',\n",
    "               'posted','pm','day','people','time','over','dont','today','see','family','join','testify','ceo','should','also',\n",
    "               'most','which','than','had','even','last','many','make','aiera','used','could','wrote','posted','post','much','open',\n",
    "               'may','much','only','app','story','group','vear','year','going','awan','under','crewcrew','aggregateiq',\n",
    "               'facebooks','use','apps','big','ms','ge','g','say','us','two','three','four','five','six','women','man','woman','men'\n",
    "\n",
    "               ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "533"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business\n",
      "----------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Science\n",
      "----------\n",
      "Sport\n",
      "----------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Politics\n",
      "----------\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# stop_words = list(get_stop_words('en'))         #About 900 stopwords\n",
    "# nltk_words = list(stopwords.words('english')) #About 150 stopwords\n",
    "# stop_words.extend(nltk_words)\n",
    "#top_selected = 10\n",
    "PATH_1 = '/Users/maweibin/Desktop/data intensive computing/lab3/part2/data_0425'\n",
    "PATH_2 = '/Users/maweibin/Desktop/data intensive computing/lab3/part2/data_0426'\n",
    "\n",
    "frequency_bus = []\n",
    "frequency_pol = []\n",
    "frequency_spo = []\n",
    "frequency_foo = []\n",
    "\n",
    "def clean_stopwords(clean_word):\n",
    "    output = [w for w in clean_word if w not in stop_words]\n",
    "    counts = Counter(output)\n",
    "    return counts\n",
    "    \n",
    "def loadtxt(filenames):\n",
    "    with open(filenames, 'r') as f:\n",
    "        x = f.readlines()\n",
    "#         print(x[0])\n",
    "        x[0] = x[0].lower()\n",
    "        #print(x[0])\n",
    "#         print(x)\n",
    "        filter_word = '[^a-zA-Z \\']'\n",
    "        clean_word = re.sub(filter_word, '', x[0])\n",
    "        data = clean_word.split()  #['...'] --> ['','','',...]\n",
    "#         for i in range(len(data)):\n",
    "#             clean_data = data[i].strip('\\'')\n",
    "        #print(data)\n",
    "        return clean_stopwords(data)\n",
    "\n",
    "def key_words(path):\n",
    "    frequency_bus = []\n",
    "    frequency_pol = []\n",
    "    frequency_spo = []\n",
    "    frequency_foo = []\n",
    "    for dir in os.listdir(path):\n",
    "        if dir == 'business':\n",
    "            business_path = os.path.join(path,'business')\n",
    "            os.chdir(business_path)\n",
    "            #count dataset\n",
    "            filenames = sorted(glob.glob('business*.txt'))\n",
    "            #print(filenames)\n",
    "            for i in filenames:\n",
    "                frequency_bus.append(loadtxt(str(i)))\n",
    "            print('Business')\n",
    "            print('-'*10)\n",
    "#             print(frequency_bus)\n",
    "            print('-'*100)\n",
    "            \n",
    "        elif dir == 'politics':\n",
    "            business_path = os.path.join(path,'politics')\n",
    "            os.chdir(business_path)\n",
    "            #count dataset\n",
    "            filenames = sorted(glob.glob('politics*.txt'))\n",
    "            for i in filenames:\n",
    "                frequency_pol.append(loadtxt(str(i)))\n",
    "            print('Politics')\n",
    "            print('-'*10)\n",
    "#             print(frequency_pol)\n",
    "            print('-'*100)\n",
    "            \n",
    "        elif dir == 'sports':\n",
    "            business_path = os.path.join(path,'sports')\n",
    "            os.chdir(business_path)\n",
    "            #count dataset\n",
    "            filenames = sorted(glob.glob('sports*.txt'))\n",
    "            for i in filenames:\n",
    "                frequency_spo.append(loadtxt(str(i)))\n",
    "            print('Sport')\n",
    "            print('-'*10)\n",
    "#             print(frequency_spo)\n",
    "            print('-'*100)\n",
    "        \n",
    "        elif dir =='science':\n",
    "            business_path = os.path.join(path,'science')\n",
    "            os.chdir(business_path)\n",
    "            #count dataset\n",
    "            filenames = sorted(glob.glob('science*.txt'))\n",
    "            for i in filenames:\n",
    "                frequency_foo.append(loadtxt(str(i)))\n",
    "            print('Science')\n",
    "            print('-'*10)\n",
    "#             print(frequency_foo)\n",
    "    return frequency_bus, frequency_pol, frequency_spo ,frequency_foo\n",
    "\n",
    "frequency_bus, frequency_pol, frequency_spo ,frequency_foo = key_words(PATH_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dollar', 31],\n",
       " ['food', 18],\n",
       " ['bank', 17],\n",
       " ['cable', 23],\n",
       " ['reports', 26],\n",
       " ['countrywide', 37],\n",
       " ['bitcoin', 22],\n",
       " ['icahn', 36],\n",
       " ['america', 27],\n",
       " ['times', 18],\n",
       " ['director', 33],\n",
       " ['cohen', 18],\n",
       " ['banks', 17],\n",
       " ['herbalife', 32],\n",
       " ['shares', 30],\n",
       " ['law', 31],\n",
       " ['class', 31],\n",
       " ['business', 31],\n",
       " ['goldman', 22],\n",
       " ['mf', 20],\n",
       " ['rule', 31],\n",
       " ['warner', 21],\n",
       " ['apollo', 29],\n",
       " ['loans', 22],\n",
       " ['tourre', 21],\n",
       " ['dealbook', 18],\n",
       " ['deal', 18],\n",
       " ['galleon', 28],\n",
       " ['morgan', 27],\n",
       " ['public', 13],\n",
       " ['c', 27],\n",
       " ['bloomberg', 21],\n",
       " ['negroni', 27],\n",
       " ['exchange', 20],\n",
       " ['subprime', 27],\n",
       " ['european', 27],\n",
       " ['charter', 26],\n",
       " ['venture', 26],\n",
       " ['news', 20],\n",
       " ['rengan', 26],\n",
       " ['proposals', 26],\n",
       " ['rate', 26],\n",
       " ['falcone', 25],\n",
       " ['facebook', 18],\n",
       " ['jobs', 31],\n",
       " ['students', 10],\n",
       " ['carlyle', 25],\n",
       " ['global', 25],\n",
       " ['money', 20],\n",
       " ['jpmorgan', 20],\n",
       " ['unemployment', 25],\n",
       " ['chess', 25],\n",
       " ['zillow', 25],\n",
       " ['read', 24],\n",
       " ['bill', 32],\n",
       " ['million', 11],\n",
       " ['reuters', 17],\n",
       " ['information', 12],\n",
       " ['equity', 24],\n",
       " ['karpel', 24],\n",
       " ['part', 24],\n",
       " ['gupta', 20],\n",
       " ['general', 24],\n",
       " ['percent', 11],\n",
       " ['masters', 23],\n",
       " ['cooper', 105],\n",
       " ['gensler', 23],\n",
       " ['el', 23],\n",
       " ['private', 23],\n",
       " ['ally', 23],\n",
       " ['gooch', 23],\n",
       " ['complaints', 23],\n",
       " ['waste', 23],\n",
       " ['nook', 23],\n",
       " ['plan', 23],\n",
       " ['pay', 22],\n",
       " ['data', 25],\n",
       " ['deutsche', 18],\n",
       " ['wall', 24],\n",
       " ['street', 12],\n",
       " ['voting', 22],\n",
       " ['insurance', 13],\n",
       " ['car', 11],\n",
       " ['office', 22],\n",
       " ['york', 20],\n",
       " ['siemens', 22],\n",
       " ['lenders', 22],\n",
       " ['funds', 19],\n",
       " ['pension', 21],\n",
       " ['sec', 21],\n",
       " ['barclays', 21],\n",
       " ['central', 21],\n",
       " ['diesel', 21],\n",
       " ['rolet', 21],\n",
       " ['market', 21],\n",
       " ['agency', 21],\n",
       " ['patents', 21],\n",
       " ['summer', 21],\n",
       " ['trulia', 21],\n",
       " ['businesses', 20],\n",
       " ['hong', 20],\n",
       " ['financial', 18],\n",
       " ['kinder', 20],\n",
       " ['children', 24],\n",
       " ['schools', 20],\n",
       " ['tax', 23],\n",
       " ['lending', 20],\n",
       " ['creative', 20],\n",
       " ['barnes', 20],\n",
       " ['debt', 19],\n",
       " ['gilman', 19],\n",
       " ['dr', 11],\n",
       " ['bidders', 19],\n",
       " ['rates', 19],\n",
       " ['stanley', 19],\n",
       " ['journal', 18],\n",
       " ['school', 10],\n",
       " ['rental', 19],\n",
       " ['google', 19],\n",
       " ['comcast', 19],\n",
       " ['sentence', 19],\n",
       " ['council', 19],\n",
       " ['safety', 19],\n",
       " ['spending', 19],\n",
       " ['fund', 19],\n",
       " ['trading', 18],\n",
       " ['taro', 19],\n",
       " ['inflation', 19],\n",
       " ['conspiracy', 19],\n",
       " ['banking', 18],\n",
       " ['paso', 18],\n",
       " ['billion', 18],\n",
       " ['companies', 18],\n",
       " ['writes', 18],\n",
       " ['company', 10],\n",
       " ['regulators', 18],\n",
       " ['wage', 18],\n",
       " ['lehman', 18],\n",
       " ['sherwood', 18],\n",
       " ['clayton', 18],\n",
       " ['london', 18],\n",
       " ['metlife', 18],\n",
       " ['proxy', 18],\n",
       " ['clos', 18],\n",
       " ['bennett', 18],\n",
       " ['medicare', 18],\n",
       " ['stockton', 18],\n",
       " ['board', 18],\n",
       " ['paypal', 18],\n",
       " ['directors', 18],\n",
       " ['alibaba', 18],\n",
       " ['agreement', 17],\n",
       " ['madoff', 17],\n",
       " ['martoma', 17],\n",
       " ['mullins', 17],\n",
       " ['sanders', 15],\n",
       " ['clinton', 17],\n",
       " ['applause', 58],\n",
       " ['think', 13],\n",
       " ['holt', 91],\n",
       " ['know', 15],\n",
       " ['senator', 70],\n",
       " ['secretary', 68],\n",
       " ['todd', 68],\n",
       " ['omalley', 63],\n",
       " ['president', 17],\n",
       " ['flint', 60],\n",
       " ['right', 34],\n",
       " ['need', 20],\n",
       " ['country', 32],\n",
       " ['question', 31],\n",
       " ['thank', 24],\n",
       " ['thats', 20],\n",
       " ['water', 45],\n",
       " ['mitchell', 17],\n",
       " ['care', 10],\n",
       " ['believe', 22],\n",
       " ['maddow', 42],\n",
       " ['im', 10],\n",
       " ['governor', 40],\n",
       " ['health', 10],\n",
       " ['way', 22],\n",
       " ['got', 31],\n",
       " ['campaign', 20],\n",
       " ['work', 36],\n",
       " ['talk', 22],\n",
       " ['american', 23],\n",
       " ['trump', 18],\n",
       " ['lets', 18],\n",
       " ['voters', 32],\n",
       " ['michigan', 31],\n",
       " ['lot', 24],\n",
       " ['mrs', 18],\n",
       " ['support', 26],\n",
       " ['let', 24],\n",
       " ['issue', 29],\n",
       " ['blankenship', 28],\n",
       " ['back', 13],\n",
       " ['every', 27],\n",
       " ['returns', 27],\n",
       " ['trade', 27],\n",
       " ['cruz', 18],\n",
       " ['weve', 17],\n",
       " ['lemon', 25],\n",
       " ['system', 19],\n",
       " ['andrea', 25],\n",
       " ['things', 18],\n",
       " ['maggie', 21],\n",
       " ['states', 23],\n",
       " ['done', 25],\n",
       " ['government', 16],\n",
       " ['tell', 24],\n",
       " ['pat', 24],\n",
       " ['absolutely', 24],\n",
       " ['progressive', 24],\n",
       " ['debate', 17],\n",
       " ['first', 10],\n",
       " ['democrats', 23],\n",
       " ['gun', 19],\n",
       " ['state', 15],\n",
       " ['lead', 22],\n",
       " ['democratic', 18],\n",
       " ['obama', 21],\n",
       " ['carson', 18],\n",
       " ['bush', 19],\n",
       " ['christie', 22],\n",
       " ['franken', 22],\n",
       " ['world', 22],\n",
       " ['issues', 22],\n",
       " ['rubio', 22],\n",
       " ['united', 21],\n",
       " ['good', 17],\n",
       " ['end', 21],\n",
       " ['dole', 21],\n",
       " ['kasich', 18],\n",
       " ['political', 21],\n",
       " ['veterans', 21],\n",
       " ['try', 18],\n",
       " ['tonight', 20],\n",
       " ['youve', 20],\n",
       " ['actually', 20],\n",
       " ['walker', 16],\n",
       " ['economy', 20],\n",
       " ['detroit', 19],\n",
       " ['working', 19],\n",
       " ['act', 19],\n",
       " ['break', 19],\n",
       " ['romney', 19],\n",
       " ['getting', 18],\n",
       " ['police', 15],\n",
       " ['convention', 18],\n",
       " ['muslims', 18],\n",
       " ['ryan', 18],\n",
       " ['americans', 14],\n",
       " ['hard', 18],\n",
       " ['isis', 18],\n",
       " ['pataki', 18],\n",
       " ['latino', 18],\n",
       " ['beijing', 17],\n",
       " ['harpootlian', 17],\n",
       " ['candidates', 17],\n",
       " ['kids', 17],\n",
       " ['jackson', 23],\n",
       " ['vs', 35],\n",
       " ['sampras', 14],\n",
       " ['gyau', 33],\n",
       " ['mets', 14],\n",
       " ['knicks', 13],\n",
       " ['sampson', 31],\n",
       " ['ware', 31],\n",
       " ['del', 30],\n",
       " ['super', 29],\n",
       " ['bowl', 18],\n",
       " ['kuhn', 28],\n",
       " ['holyfield', 28],\n",
       " ['marinovich', 26],\n",
       " ['hall', 25],\n",
       " ['sca', 25],\n",
       " ['collins', 19],\n",
       " ['lofton', 25],\n",
       " ['fred', 24],\n",
       " ['game', 13],\n",
       " ['tour', 24],\n",
       " ['steinbrenner', 13],\n",
       " ['cap', 24],\n",
       " ['murray', 24],\n",
       " ['cubs', 23],\n",
       " ['lendl', 13],\n",
       " ['paul', 23],\n",
       " ['reggie', 23],\n",
       " ['hewitt', 22],\n",
       " ['potro', 22],\n",
       " ['yashin', 22],\n",
       " ['league', 13],\n",
       " ['puckett', 22],\n",
       " ['kompany', 22],\n",
       " ['mcrae', 22],\n",
       " ['tomba', 22],\n",
       " ['ervins', 22],\n",
       " ['mcdaniel', 22],\n",
       " ['blatt', 22],\n",
       " ['tennis', 14],\n",
       " ['faldo', 21],\n",
       " ['drills', 21],\n",
       " ['team', 18],\n",
       " ['shoemaker', 21],\n",
       " ['south', 21],\n",
       " ['university', 13],\n",
       " ['reeves', 20],\n",
       " ['messier', 20],\n",
       " ['cup', 20],\n",
       " ['salmon', 19],\n",
       " ['vincent', 19],\n",
       " ['bonilla', 20],\n",
       " ['whitfield', 20],\n",
       " ['rodriguez', 19],\n",
       " ['players', 14],\n",
       " ['robinson', 19],\n",
       " ['concussion', 19],\n",
       " ['gasol', 19],\n",
       " ['triplecast', 19],\n",
       " ['pascual', 19],\n",
       " ['de', 19],\n",
       " ['berhalter', 19],\n",
       " ['ecclestone', 19],\n",
       " ['stadium', 18],\n",
       " ['williams', 11],\n",
       " ['johnson', 16],\n",
       " ['runs', 18],\n",
       " ['kramer', 18],\n",
       " ['indurain', 18],\n",
       " ['pistons', 18],\n",
       " ['yards', 14],\n",
       " ['yankees', 18],\n",
       " ['ski', 18],\n",
       " ['giants', 14],\n",
       " ['moore', 14],\n",
       " ['lee', 17],\n",
       " ['finished', 17],\n",
       " ['jaziri', 17],\n",
       " ['mears', 17],\n",
       " ['girardi', 17],\n",
       " ['cigar', 17],\n",
       " ['gatt', 16],\n",
       " ['grand', 17],\n",
       " ['points', 13],\n",
       " ['kleinman', 17],\n",
       " ['handley', 17],\n",
       " ['colts', 17],\n",
       " ['djokovic', 17],\n",
       " ['report', 10],\n",
       " ['auburn', 16],\n",
       " ['circles', 16],\n",
       " ['lost', 16],\n",
       " ['eagles', 14],\n",
       " ['islanders', 16],\n",
       " ['arazi', 16],\n",
       " ['agassi', 16],\n",
       " ['beat', 16],\n",
       " ['bell', 16],\n",
       " ['statistics', 16],\n",
       " ['rating', 16],\n",
       " ['dallas', 15],\n",
       " ['chassis', 15],\n",
       " ['jets', 15],\n",
       " ['columbia', 15],\n",
       " ['quarterback', 15],\n",
       " ['ojeda', 15],\n",
       " ['selig', 15],\n",
       " ['city', 15],\n",
       " ['fernandez', 15],\n",
       " ['cabrera', 15],\n",
       " ['jordan', 15],\n",
       " ['muster', 15],\n",
       " ['damon', 15],\n",
       " ['slam', 15],\n",
       " ['cash', 15],\n",
       " ['field', 15],\n",
       " ['dolan', 15],\n",
       " ['baseball', 15],\n",
       " ['committee', 15],\n",
       " ['owners', 12],\n",
       " ['gilbert', 15],\n",
       " ['matchup', 15],\n",
       " ['sabathia', 14],\n",
       " ['weaknesses', 14],\n",
       " ['outlook', 14],\n",
       " ['espn', 14],\n",
       " ['polian', 14],\n",
       " ['ravens', 14],\n",
       " ['barkley', 14],\n",
       " ['olympic', 14],\n",
       " ['athletes', 14],\n",
       " ['syracuse', 14],\n",
       " ['ball', 14],\n",
       " ['jagge', 14],\n",
       " ['long', 14],\n",
       " ['island', 14],\n",
       " ['soccer', 14],\n",
       " ['old', 14],\n",
       " ['firm', 13],\n",
       " ['round', 14],\n",
       " ['really', 14],\n",
       " ['becker', 14],\n",
       " ['valvano', 14],\n",
       " ['waivers', 14],\n",
       " ['fans', 14],\n",
       " ['phillies', 13],\n",
       " ['fish', 11],\n",
       " ['fischer', 13],\n",
       " ['clubs', 13],\n",
       " ['killer', 13],\n",
       " ['instinct', 13],\n",
       " ['strike', 13],\n",
       " ['smith', 13],\n",
       " ['thorn', 13],\n",
       " ['boat', 13],\n",
       " ['pippen', 13],\n",
       " ['surez', 13],\n",
       " ['coverage', 13],\n",
       " ['melido', 13],\n",
       " ['course', 13],\n",
       " ['pretorius', 13],\n",
       " ['saberhagen', 13],\n",
       " ['advantage', 13],\n",
       " ['img', 13],\n",
       " ['bulls', 13],\n",
       " ['horse', 13],\n",
       " ['reds', 13],\n",
       " ['wilander', 13],\n",
       " ['play', 13],\n",
       " ['spira', 13],\n",
       " ['commissioner', 13],\n",
       " ['champion', 13],\n",
       " ['garcia', 13],\n",
       " ['pain', 12],\n",
       " ['bike', 52],\n",
       " ['bread', 38],\n",
       " ['math', 36],\n",
       " ['yogurt', 36],\n",
       " ['lane', 33],\n",
       " ['cancer', 12],\n",
       " ['lanes', 30],\n",
       " ['exercise', 18],\n",
       " ['cosleeping', 29],\n",
       " ['sleep', 12],\n",
       " ['corn', 28],\n",
       " ['whole', 28],\n",
       " ['giraffes', 27],\n",
       " ['coffee', 27],\n",
       " ['carbon', 26],\n",
       " ['lgbt', 24],\n",
       " ['running', 18],\n",
       " ['hair', 11],\n",
       " ['drinking', 24],\n",
       " ['alcohol', 24],\n",
       " ['stress', 24],\n",
       " ['sugar', 23],\n",
       " ['giraffe', 22],\n",
       " ['alvarez', 22],\n",
       " ['love', 22],\n",
       " ['pie', 22],\n",
       " ['breast', 21],\n",
       " ['dreams', 21],\n",
       " ['greek', 21],\n",
       " ['dogs', 16],\n",
       " ['coke', 13],\n",
       " ['texting', 20],\n",
       " ['selfcontrol', 20],\n",
       " ['caffeine', 19],\n",
       " ['anxiety', 19],\n",
       " ['vitamins', 19],\n",
       " ['measles', 19],\n",
       " ['forest', 18],\n",
       " ['metabolizers', 18],\n",
       " ['chemicals', 18],\n",
       " ['circumcision', 18],\n",
       " ['academy', 17],\n",
       " ['medical', 17],\n",
       " ['training', 14],\n",
       " ['bed', 17],\n",
       " ['formula', 17],\n",
       " ['community', 16],\n",
       " ['counselors', 16],\n",
       " ['parents', 10],\n",
       " ['white', 16],\n",
       " ['patients', 10],\n",
       " ['trials', 16],\n",
       " ['air', 16],\n",
       " ['god', 15],\n",
       " ['risk', 15],\n",
       " ['fast', 15],\n",
       " ['cocacola', 13],\n",
       " ['obesity', 15],\n",
       " ['bird', 15],\n",
       " ['brain', 15],\n",
       " ['breads', 15],\n",
       " ['materials', 15],\n",
       " ['celiac', 15],\n",
       " ['diabetes', 14],\n",
       " ['nursing', 10],\n",
       " ['green', 15],\n",
       " ['house', 15],\n",
       " ['lice', 15],\n",
       " ['treatment', 15],\n",
       " ['groups', 14],\n",
       " ['rna', 14],\n",
       " ['run', 12],\n",
       " ['weight', 14],\n",
       " ['birds', 14],\n",
       " ['compulsive', 14],\n",
       " ['traffic', 14],\n",
       " ['milk', 14],\n",
       " ['type', 14],\n",
       " ['mental', 14],\n",
       " ['test', 14],\n",
       " ['dream', 14],\n",
       " ['mammary', 14],\n",
       " ['heart', 13],\n",
       " ['life', 13],\n",
       " ['chemistry', 13],\n",
       " ['suicide', 13],\n",
       " ['depression', 13],\n",
       " ['home', 12],\n",
       " ['marathon', 13],\n",
       " ['infections', 13],\n",
       " ['cycling', 13],\n",
       " ['study', 10],\n",
       " ['baby', 13],\n",
       " ['grain', 13],\n",
       " ['flour', 13],\n",
       " ['wholegrain', 13],\n",
       " ['autopsy', 13],\n",
       " ['birthday', 13],\n",
       " ['organic', 13],\n",
       " ['score', 13],\n",
       " ['amanda', 13],\n",
       " ['cards', 13],\n",
       " ['cholesterol', 13],\n",
       " ['human', 12],\n",
       " ['slow', 12],\n",
       " ['cardiovascular', 12],\n",
       " ['cyclists', 12],\n",
       " ['interval', 12],\n",
       " ['texters', 12],\n",
       " ['runners', 11],\n",
       " ['babies', 12],\n",
       " ['fiber', 12],\n",
       " ['grains', 12],\n",
       " ['disease', 11],\n",
       " ['st', 12],\n",
       " ['drinks', 12],\n",
       " ['homes', 12],\n",
       " ['card', 12],\n",
       " ['drug', 11],\n",
       " ['adults', 11],\n",
       " ['tsimane', 11],\n",
       " ['coronary', 11],\n",
       " ['menopause', 11],\n",
       " ['injuries', 11],\n",
       " ['energy', 11],\n",
       " ['physical', 11],\n",
       " ['girls', 11],\n",
       " ['crosstraining', 11],\n",
       " ['dementia', 11],\n",
       " ['sara', 11],\n",
       " ['hospital', 11],\n",
       " ['patient', 10],\n",
       " ['nightingale', 11],\n",
       " ['head', 10],\n",
       " ['tumors', 11],\n",
       " ['virus', 11],\n",
       " ['dad', 11],\n",
       " ['gmos', 11],\n",
       " ['pollution', 11],\n",
       " ['lung', 11],\n",
       " ['eating', 11],\n",
       " ['diet', 11],\n",
       " ['alzheimers', 11],\n",
       " ['industry', 10],\n",
       " ['high', 10],\n",
       " ['blood', 10],\n",
       " ['chemical', 10],\n",
       " ['ive', 10],\n",
       " ['months', 10],\n",
       " ['nose', 10],\n",
       " ['counselor', 10],\n",
       " ['mathanxious', 10],\n",
       " ['seconds', 10],\n",
       " ['calories', 10],\n",
       " ['balance', 10],\n",
       " ['activity', 10],\n",
       " ['research', 10],\n",
       " ['symptoms', 10],\n",
       " ['meal', 10],\n",
       " ['nurses', 10],\n",
       " ['active', 10],\n",
       " ['nurse', 10],\n",
       " ['feed', 10],\n",
       " ['medicine', 10],\n",
       " ['oil', 10],\n",
       " ['art', 10],\n",
       " ['pecan', 10],\n",
       " ['bod', 10],\n",
       " ['foods', 10],\n",
       " ['eggs', 10]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######get top words of each category\n",
    "def top_words_category(freq_list_category):\n",
    "    list_business = []\n",
    "    for i in range(len(freq_list_category)):\n",
    "        list_business.extend((freq_list_category[i]).most_common(250))  #top words of each txt\n",
    "    sorted_list = sorted(list_business, key = lambda tup: tup[1], reverse= True) #sort\n",
    "    top_bus = sorted_list[:250]  #top words of each category\n",
    "    return top_bus\n",
    "\n",
    "def combine_top_words(p, s, b, t):\n",
    "    Total_top_words = top_words_category(p) + top_words_category(s) + top_words_category(b) + top_words_category(t)\n",
    "    return Total_top_words\n",
    "\n",
    "##total words for each category\n",
    "top_word_business = top_words_category(frequency_bus)\n",
    "top_word_politics = top_words_category(frequency_pol)\n",
    "top_word_sport = top_words_category(frequency_spo)\n",
    "top_word_travel = top_words_category(frequency_foo)\n",
    "##total words for four category\n",
    "Total_top_words = combine_top_words(frequency_bus, frequency_pol, frequency_spo, frequency_foo)\n",
    "Total_top_words = [list(v) for v in dict(Total_top_words).items()]  ####unique key words\n",
    "# Total_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_top_words_list = []\n",
    "for i in range(len(Total_top_words)):\n",
    "    Total_top_words_list.append(Total_top_words[i][0])\n",
    "# Total_top_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_data_freq(p, s, b, t):\n",
    "    Total_List_counter = p + s + b + t\n",
    "    Total_List_dict = []\n",
    "    for i in range(len(Total_List_counter)):\n",
    "        Total_List_dict.append(dict(Total_List_counter[i]))\n",
    "    return Total_List_dict\n",
    "##total list_dict of all txt files\n",
    "Total_List_dict = combined_data_freq(frequency_bus, frequency_pol, frequency_spo, frequency_foo)\n",
    "# Total_List_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dollar</th>\n",
       "      <th>food</th>\n",
       "      <th>bank</th>\n",
       "      <th>cable</th>\n",
       "      <th>reports</th>\n",
       "      <th>countrywide</th>\n",
       "      <th>bitcoin</th>\n",
       "      <th>icahn</th>\n",
       "      <th>america</th>\n",
       "      <th>times</th>\n",
       "      <th>...</th>\n",
       "      <th>active</th>\n",
       "      <th>nurse</th>\n",
       "      <th>feed</th>\n",
       "      <th>medicine</th>\n",
       "      <th>oil</th>\n",
       "      <th>art</th>\n",
       "      <th>pecan</th>\n",
       "      <th>bod</th>\n",
       "      <th>foods</th>\n",
       "      <th>eggs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 606 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  dollar food bank cable reports countrywide bitcoin icahn america times ...   \\\n",
       "0      0    0    6     0      14           0       0     0       3     7 ...    \n",
       "1      0    0    1    38       0           0       0     0       0     0 ...    \n",
       "2      0    0    0     0       0           0       0     0       0     0 ...    \n",
       "3      0    0    4     0       0           0       0     0       0     2 ...    \n",
       "4      0    0    0     0       0           0       0     0       0     1 ...    \n",
       "\n",
       "  active nurse feed medicine oil art pecan bod foods eggs  \n",
       "0      0     0    0        0   0   0     0   0     0    0  \n",
       "1      0     0    0        0   0   0     0   0     0    0  \n",
       "2      0     0    0        1   0   0     0   0     0    0  \n",
       "3      0     0    0        0   0   0     0   0     0    0  \n",
       "4      0     0    0        0   0   0     0   0     0    0  \n",
       "\n",
       "[5 rows x 606 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##get each column name\n",
    "column_name_list = []\n",
    "for i in range(len(Total_top_words)):\n",
    "    column_name_list.append(Total_top_words[i][0])\n",
    "    \n",
    "##get each row name\n",
    "row_name_list=[]\n",
    "for i in range(len(Total_List_dict)):\n",
    "    row_name_list.append(i)\n",
    "\n",
    "\n",
    "Data_Frame = pd.DataFrame(index = row_name_list, columns = Total_top_words_list)\n",
    "# Data_Frame\n",
    "\n",
    "for col in range(len(column_name_list)): #for column\n",
    "    for row in range(len(row_name_list)):  #for row\n",
    "        if Total_List_dict[row].get(column_name_list[col]) is None:\n",
    "            Data_Frame.loc[row, column_name_list[col]] = 0\n",
    "        else:\n",
    "            Data_Frame.loc[row, column_name_list[col]] = Total_List_dict[row].get(column_name_list[col])\n",
    "pd.DataFrame.head(Data_Frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dollar</th>\n",
       "      <th>food</th>\n",
       "      <th>bank</th>\n",
       "      <th>cable</th>\n",
       "      <th>reports</th>\n",
       "      <th>countrywide</th>\n",
       "      <th>bitcoin</th>\n",
       "      <th>icahn</th>\n",
       "      <th>america</th>\n",
       "      <th>times</th>\n",
       "      <th>...</th>\n",
       "      <th>nurse</th>\n",
       "      <th>feed</th>\n",
       "      <th>medicine</th>\n",
       "      <th>oil</th>\n",
       "      <th>art</th>\n",
       "      <th>pecan</th>\n",
       "      <th>bod</th>\n",
       "      <th>foods</th>\n",
       "      <th>eggs</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 607 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  dollar food bank cable reports countrywide bitcoin icahn america times  \\\n",
       "0      0    0    6     0      14           0       0     0       3     7   \n",
       "1      0    0    1    38       0           0       0     0       0     0   \n",
       "2      0    0    0     0       0           0       0     0       0     0   \n",
       "3      0    0    4     0       0           0       0     0       0     2   \n",
       "4      0    0    0     0       0           0       0     0       0     1   \n",
       "\n",
       "   ...  nurse feed medicine oil art pecan bod foods eggs label  \n",
       "0  ...      0    0        0   0   0     0   0     0    0   0.0  \n",
       "1  ...      0    0        0   0   0     0   0     0    0   0.0  \n",
       "2  ...      0    0        1   0   0     0   0     0    0   0.0  \n",
       "3  ...      0    0        0   0   0     0   0     0    0   0.0  \n",
       "4  ...      0    0        0   0   0     0   0     0    0   0.0  \n",
       "\n",
       "[5 rows x 607 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#frequency_bus -> 0\n",
    "#frequency_pol -> 1\n",
    "#frequency_spo -> 2\n",
    "#frequency_foo -> 3\n",
    "a = len(frequency_bus) - 1\n",
    "b = len(frequency_bus) + len(frequency_pol) - 1\n",
    "c = len(frequency_bus) + len(frequency_pol) + len(frequency_spo) - 1\n",
    "\n",
    "Data_Frame.loc[0:a,'label'] = 0\n",
    "Data_Frame.loc[a+1:b,'label'] = 1\n",
    "Data_Frame.loc[b+1:c,'label'] = 2\n",
    "Data_Frame.loc[c+1:,'label'] = 3\n",
    "pd.DataFrame.head(Data_Frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##save as .csv\n",
    "Data_Frame.to_csv('/Users/maweibin/Desktop/data intensive computing/lab3/part2/data_0425/dataset.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithms\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 607)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data_Frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maweibin/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "X = Data_Frame.iloc[:, 0:Data_Frame.shape[1]-1].values\n",
    "y = Data_Frame.iloc[:, Data_Frame.shape[1]-1].values\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 111)\n",
    "\n",
    "# # # #Feature Scaling\n",
    "# sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "# X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Normalizing\n",
    "normalizer = Normalizer()\n",
    "X_train = normalizer.fit_transform(X_train)\n",
    "X_test = normalizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.75 %\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_predknn = knn.predict(X_test)\n",
    "\n",
    "acc_knn = accuracy_score(y_test,y_predknn)*100\n",
    "print(round(acc_knn,2,), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.0 %\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_predlog = logreg.predict(X_test)\n",
    "\n",
    "acc_log = accuracy_score(y_test,y_predlog)*100\n",
    "print(round(acc_log,2,), \"%\")\n",
    "\n",
    "# C = [50, 20, 10, 1, .1, .001, .00001]\n",
    "# for c in C:\n",
    "#     clf = LogisticRegression(penalty='l2', C=c)\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     print('C:', c)\n",
    "#     #print('Coefficient of each feature:', clf.coef_)\n",
    "#     print('Training accuracy:', clf.score(X_train, y_train))\n",
    "#     print('Test accuracy:', clf.score(X_test, y_test))\n",
    "#     print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.25 %\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "random_forest = RandomForestClassifier(n_estimators=200)\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "y_predrf = random_forest.predict(X_test)\n",
    "\n",
    "random_forest.score(X_train, y_train)\n",
    "acc_random_forest = accuracy_score(y_test,y_predrf)*100\n",
    "print(round(acc_random_forest,2,), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.5 %\n"
     ]
    }
   ],
   "source": [
    "# SVM \n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_predsvm = classifier.predict(X_test)\n",
    "\n",
    "acc_svm = accuracy_score(y_test,y_predsvm)*100\n",
    "print(round(acc_svm,2,), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85.00</th>\n",
       "      <td>Logistic Regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84.25</th>\n",
       "      <td>Random Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83.50</th>\n",
       "      <td>SVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70.75</th>\n",
       "      <td>KNN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model\n",
       "Accuracy                     \n",
       "85.00     Logistic Regression\n",
       "84.25           Random Forest\n",
       "83.50                     SVM\n",
       "70.75                     KNN"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Model': ['KNN', 'Logistic Regression', 'Random Forest', 'SVM'],\n",
    "    'Accuracy': [acc_knn, acc_log, acc_random_forest, acc_svm]})\n",
    "result_df = results.sort_values(by='Accuracy', ascending=False)\n",
    "result_df = result_df.set_index('Accuracy')\n",
    "result_df.head(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business\n",
      "----------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Science\n",
      "----------\n",
      "Sport\n",
      "----------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Politics\n",
      "----------\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "pred_freq_bus, pred_freq_pol, pred_freq_spo ,pred_freq_foo = key_words(PATH_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "######get top words of each category\n",
    "##total words for each category\n",
    "top_word_business = top_words_category(frequency_bus)\n",
    "top_word_politics = top_words_category(frequency_pol)\n",
    "top_word_sport = top_words_category(frequency_spo)\n",
    "top_word_travel = top_words_category(frequency_foo)\n",
    "##total words for four category\n",
    "Total_top_words1 = combine_top_words(pred_freq_bus, pred_freq_pol, pred_freq_spo, pred_freq_foo)\n",
    "Total_top_words1 = [list(v) for v in dict(Total_top_words1).items()]  ####unique key words\n",
    "# Total_top_words1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##total list_dict of all txt files\n",
    "pred_total_List_dict = combined_data_freq(pred_freq_bus, pred_freq_pol, pred_freq_spo ,pred_freq_foo)\n",
    "# pred_total_List_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dollar</th>\n",
       "      <th>food</th>\n",
       "      <th>bank</th>\n",
       "      <th>cable</th>\n",
       "      <th>reports</th>\n",
       "      <th>countrywide</th>\n",
       "      <th>bitcoin</th>\n",
       "      <th>icahn</th>\n",
       "      <th>america</th>\n",
       "      <th>times</th>\n",
       "      <th>...</th>\n",
       "      <th>active</th>\n",
       "      <th>nurse</th>\n",
       "      <th>feed</th>\n",
       "      <th>medicine</th>\n",
       "      <th>oil</th>\n",
       "      <th>art</th>\n",
       "      <th>pecan</th>\n",
       "      <th>bod</th>\n",
       "      <th>foods</th>\n",
       "      <th>eggs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 606 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  dollar food bank cable reports countrywide bitcoin icahn america times ...   \\\n",
       "0      0    0    0     0       0           0       0     0       0     2 ...    \n",
       "1      0    0    0     0       0           0       0     0       0     3 ...    \n",
       "2      0    0    0     0       0           0       0     0       0     2 ...    \n",
       "3      0    0    0     0       0           0       0     0       0     0 ...    \n",
       "4      0    0    0     0       0           0       0     0       0     0 ...    \n",
       "\n",
       "  active nurse feed medicine oil art pecan bod foods eggs  \n",
       "0      0     0    0        0   0   0     0   0     0    0  \n",
       "1      0     0    0        0   0   0     0   0     0    0  \n",
       "2      0     0    0        0   0   0     0   0     0    0  \n",
       "3      0     0    0        0   0   0     0   0     0    0  \n",
       "4      0     0    0        0   0   0     0   0     0    0  \n",
       "\n",
       "[5 rows x 606 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##get each column name\n",
    "column_name_list = []\n",
    "for i in range(len(Total_top_words)):\n",
    "    column_name_list.append(Total_top_words[i][0])\n",
    "    \n",
    "##get each row name\n",
    "row_name_list=[]\n",
    "for i in range(len(pred_total_List_dict)):\n",
    "    row_name_list.append(i)\n",
    "\n",
    "\n",
    "Data_Frame_pred = pd.DataFrame(index = row_name_list, columns = Total_top_words_list)\n",
    "# Data_Frame_pred\n",
    "\n",
    "for col in range(len(column_name_list)): #for column\n",
    "    for row in range(len(row_name_list)):  #for row\n",
    "        if pred_total_List_dict[row].get(column_name_list[col]) is None:\n",
    "            Data_Frame_pred.loc[row, column_name_list[col]] = 0\n",
    "        else:\n",
    "            Data_Frame_pred.loc[row, column_name_list[col]] = pred_total_List_dict[row].get(column_name_list[col])\n",
    "pd.DataFrame.head(Data_Frame_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dollar</th>\n",
       "      <th>food</th>\n",
       "      <th>bank</th>\n",
       "      <th>cable</th>\n",
       "      <th>reports</th>\n",
       "      <th>countrywide</th>\n",
       "      <th>bitcoin</th>\n",
       "      <th>icahn</th>\n",
       "      <th>america</th>\n",
       "      <th>times</th>\n",
       "      <th>...</th>\n",
       "      <th>nurse</th>\n",
       "      <th>feed</th>\n",
       "      <th>medicine</th>\n",
       "      <th>oil</th>\n",
       "      <th>art</th>\n",
       "      <th>pecan</th>\n",
       "      <th>bod</th>\n",
       "      <th>foods</th>\n",
       "      <th>eggs</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 607 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  dollar food bank cable reports countrywide bitcoin icahn america times  \\\n",
       "0      0    0    0     0       0           0       0     0       0     2   \n",
       "1      0    0    0     0       0           0       0     0       0     3   \n",
       "2      0    0    0     0       0           0       0     0       0     2   \n",
       "3      0    0    0     0       0           0       0     0       0     0   \n",
       "4      0    0    0     0       0           0       0     0       0     0   \n",
       "\n",
       "   ...  nurse feed medicine oil art pecan bod foods eggs label  \n",
       "0  ...      0    0        0   0   0     0   0     0    0   0.0  \n",
       "1  ...      0    0        0   0   0     0   0     0    0   0.0  \n",
       "2  ...      0    0        0   0   0     0   0     0    0   0.0  \n",
       "3  ...      0    0        0   0   0     0   0     0    0   0.0  \n",
       "4  ...      0    0        0   0   0     0   0     0    0   0.0  \n",
       "\n",
       "[5 rows x 607 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#frequency_bus -> 0\n",
    "#frequency_pol -> 1\n",
    "#frequency_spo -> 2\n",
    "#frequency_foo -> 3\n",
    "a = len(pred_freq_bus) - 1\n",
    "b = len(pred_freq_bus) + len(pred_freq_pol) - 1\n",
    "c = len(pred_freq_bus) + len(pred_freq_pol) + len(pred_freq_spo) - 1\n",
    "Data_Frame_pred.loc[0:a,'label'] = 0\n",
    "Data_Frame_pred.loc[a+1:b,'label'] = 1\n",
    "Data_Frame_pred.loc[b+1:c,'label'] = 2\n",
    "Data_Frame_pred.loc[c+1:,'label'] = 3\n",
    "pd.DataFrame.head(Data_Frame_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #Randoming the rows\n",
    "# Data_Frame_pred = Data_Frame_pred.iloc[np.random.permutation(len(Data_Frame_pred))]\n",
    "# Data_Frame_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "x_new = Data_Frame_pred.iloc[:,0:Data_Frame_pred.shape[1]-1].values\n",
    "y_new_pred = Data_Frame_pred.iloc[:,Data_Frame_pred.shape[1]-1].values\n",
    "\n",
    "# Feature Scaling\n",
    "# sc = StandardScaler()\n",
    "# sc = preprocessing.StandardScaler().fit(X)\n",
    "# # X_train = sc._transform(X_train)\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "# x_new = sc.transform(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing\n",
    "from sklearn.preprocessing import Normalizer\n",
    "# normalizer = Normalizer()\n",
    "# X_train = normalizer.fit_transform(X_train)\n",
    "# X_test = normalizer.transform(X_test)\n",
    "x_new = normalizer.transform(x_new)\n",
    "# x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.0 %\n"
     ]
    }
   ],
   "source": [
    "##KNN\n",
    "y_pred_knn = knn.predict(x_new)\n",
    "acc_knn1 = accuracy_score(y_pred_knn, y_new_pred)*100\n",
    "print(round(acc_knn1,2,), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.0 %\n"
     ]
    }
   ],
   "source": [
    "##Logistic Regression\n",
    "y_pred_log = logreg.predict(x_new)\n",
    "acc_log1 = accuracy_score(y_pred_log, y_new_pred)*100\n",
    "print(round(acc_log1,2,), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.5 %\n"
     ]
    }
   ],
   "source": [
    "##Random Forest\n",
    "y_pred_rf = random_forest.predict(x_new)\n",
    "acc_rf1 = accuracy_score(y_pred_rf, y_new_pred)*100\n",
    "print(round(acc_rf1,2,), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.25 %\n"
     ]
    }
   ],
   "source": [
    "##SVM\n",
    "y_pred_svm = classifier.predict(x_new)\n",
    "acc_svm1 = accuracy_score(y_pred_svm, y_new_pred)*100\n",
    "print(round(acc_svm1,2,), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>92.50</th>\n",
       "      <td>Random Forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90.00</th>\n",
       "      <td>Logistic Regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86.25</th>\n",
       "      <td>SVM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70.00</th>\n",
       "      <td>KNN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model\n",
       "Accuracy                     \n",
       "92.50           Random Forest\n",
       "90.00     Logistic Regression\n",
       "86.25                     SVM\n",
       "70.00                     KNN"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Model': ['KNN', 'Logistic Regression', 'Random Forest', 'SVM'],\n",
    "    'Accuracy': [acc_knn1, acc_log1, acc_rf1, acc_svm1]})\n",
    "result_df = results.sort_values(by='Accuracy', ascending=False)\n",
    "result_df = result_df.set_index('Accuracy')\n",
    "result_df.head(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 3. 0. 3. 0. 3. 0. 3. 0. 0. 3. 0. 0. 0. 0. 0. 0. 3. 3. 0. 1. 1. 1. 1.\n",
      " 3. 1. 3. 1. 1. 1. 1. 1. 1. 3. 1. 1. 1. 1. 1. 1. 1. 3. 0. 2. 1. 3. 3. 2.\n",
      " 2. 1. 3. 2. 2. 3. 2. 3. 3. 3. 3. 2. 3. 1. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3.]\n",
      "[0. 2. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 3. 1. 3. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 3. 2. 2. 2. 2. 3. 3. 3. 3. 3. 3. 0. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3.]\n",
      "[0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 3. 1. 3. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2. 3. 3. 3. 3. 3. 3. 0. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3.]\n",
      "[0. 2. 2. 3. 0. 0. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 3. 0. 1. 1. 1. 1.\n",
      " 3. 1. 3. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 3. 2. 2. 2. 2. 3. 3. 3. 3. 3. 3. 0. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred_knn)\n",
    "print(y_pred_log)\n",
    "print(y_pred_rf)\n",
    "print(y_pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 3., 3., 3., 3., 3., 3., 3., 3.,\n",
       "       3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3., 3.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_new_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
